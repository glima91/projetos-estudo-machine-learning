{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S+P Week 3 Lesson 4 - LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX4Kg8DUTKWO"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1J15Vh_1Jih"
      },
      "source": [
        "!pip install tf-nightly-2.0-preview\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOjujz601HcS",
        "outputId": "660ded30-6800-4b22-a815-96e74d3092ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zswl7jRtGzkk"
      },
      "source": [
        "def plot_series(time, series, format=\"-\", start=0, end=None):\n",
        "    plt.plot(time[start:end], series[start:end], format)\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.grid(True)\n",
        "\n",
        "def trend(time, slope=0):\n",
        "    return slope * time\n",
        "\n",
        "def seasonal_pattern(season_time):\n",
        "    \"\"\"Just an arbitrary pattern, you can change it if you wish\"\"\"\n",
        "    return np.where(season_time < 0.4,\n",
        "                    np.cos(season_time * 2 * np.pi),\n",
        "                    1 / np.exp(3 * season_time))\n",
        "\n",
        "def seasonality(time, period, amplitude=1, phase=0):\n",
        "    \"\"\"Repeats the same pattern at each period\"\"\"\n",
        "    season_time = ((time + phase) % period) / period\n",
        "    return amplitude * seasonal_pattern(season_time)\n",
        "\n",
        "def noise(time, noise_level=1, seed=None):\n",
        "    rnd = np.random.RandomState(seed)\n",
        "    return rnd.randn(len(time)) * noise_level\n",
        "\n",
        "time = np.arange(4 * 365 + 1, dtype=\"float32\")\n",
        "baseline = 10\n",
        "series = trend(time, 0.1)  \n",
        "baseline = 10\n",
        "amplitude = 40\n",
        "slope = 0.05\n",
        "noise_level = 5\n",
        "\n",
        "# Create the series\n",
        "series = baseline + trend(time, slope) + seasonality(time, period=365, amplitude=amplitude)\n",
        "# Update with noise\n",
        "series += noise(time, noise_level, seed=42)\n",
        "\n",
        "split_time = 1000\n",
        "time_train = time[:split_time]\n",
        "x_train = series[:split_time]\n",
        "time_valid = time[split_time:]\n",
        "x_valid = series[split_time:]\n",
        "\n",
        "window_size = 20\n",
        "batch_size = 32\n",
        "shuffle_buffer_size = 1000"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sTTIOCbyShY"
      },
      "source": [
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(series)\n",
        "  dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
        "  dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))\n",
        "  dataset = dataset.batch(batch_size).prefetch(1)\n",
        "  return dataset"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeFrlhBmJJjV",
        "outputId": "d7e5669e-c56a-484d-cbe5-b91b58d87502",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for x,y in dataset:\n",
        "  print(\"tensor de com batch=32 e janela = 20\")\n",
        "  print(x)\n",
        "  print()\n",
        "  print(\"32 labels\")\n",
        "  print(y)\n",
        "  break"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor de com batch=32 e janela = 20\n",
            "tf.Tensor(\n",
            "[[ 71.0577      63.200054    61.360634    68.069664    58.755577\n",
            "   74.25012     70.71561     62.1877      55.672768    70.70246\n",
            "   63.04496     69.481316    54.984783    59.61497     62.283337\n",
            "   62.12776     59.268875    64.25051     55.40607     59.631157  ]\n",
            " [ 23.712889    26.387451    25.139708    27.971327    39.131126\n",
            "   18.217827    30.982216    19.484531    25.18608     32.98855\n",
            "   27.863678    22.15246     23.963987    30.938232    23.888557\n",
            "   28.623238    27.769768    24.28527     38.26475     30.716782  ]\n",
            " [ 21.320347    32.222126    38.251995    32.812275    20.062172\n",
            "   25.247257    34.012722    24.14989     29.917734    31.582527\n",
            "   23.085745    27.434132    11.5371      22.63349     26.50489\n",
            "   21.541426    35.955315    20.655773    25.61978     28.487524  ]\n",
            " [-11.649013    -2.2406242    5.248558   -11.178716    -9.573152\n",
            "   -6.752203   -10.269627   -16.000174    -8.391326   -14.525441\n",
            "   -7.3181014  -14.747516    -2.8571053  -14.97132    -13.105373\n",
            "   -7.8591013  -18.504087   -11.626857    -6.6340213  -21.603775  ]\n",
            " [ 40.86737     59.286106    46.734955    46.52831     46.358097\n",
            "   47.48944     45.797756    43.672028    43.82706     46.418854\n",
            "   43.886986    43.061607    47.179943    45.395092    54.212364\n",
            "   33.46019     52.195435    52.991413    36.417343    45.094387  ]\n",
            " [ 36.58728     44.171944    46.010376    54.289753    47.53197\n",
            "   44.792107    50.02539     34.813927    47.039402    49.707848\n",
            "   38.453266    51.55811     47.52543     43.75058     48.98545\n",
            "   57.16999     46.721287    47.04894     43.50735     41.55169   ]\n",
            " [ 48.98545     57.16999     46.721287    47.04894     43.50735\n",
            "   41.55169     49.94979     41.51532     46.151627    43.403996\n",
            "   48.186077    47.458817    50.977943    43.240307    44.441574\n",
            "   40.89809     43.571808    47.681534    49.58213     41.188908  ]\n",
            " [ 55.40607     59.631157    60.53453     62.086494    62.645008\n",
            "   53.02789     50.536366    64.142975    58.955788    53.083527\n",
            "   64.105736    56.444798    61.271942    55.21483     64.67567\n",
            "   62.6364      52.09588     57.672966    55.509987    58.588013  ]\n",
            " [ 30.450966    26.898535    31.563684    30.338299    27.586983\n",
            "   23.696756    20.33666     25.659061    32.15494     28.92529\n",
            "   21.608559    28.686138    29.73035     23.368555    28.541082\n",
            "   28.048655    22.028439    29.518435    30.52015     33.118732  ]\n",
            " [ 31.098982    32.022236    26.868631    25.863333    35.345516\n",
            "   37.49725     29.783985    34.584694    30.793396    25.17319\n",
            "   29.614908    34.903008    26.434551    33.833897    12.305289\n",
            "   28.903387    24.615705    22.06895     23.403429    12.384742  ]\n",
            " [ 46.594894    44.934753    52.822556    44.08511     40.594\n",
            "   39.88281     50.725815    41.3236      56.529865    37.376553\n",
            "   56.298946    48.90238     47.394512    45.184433    49.935806\n",
            "   47.783203    53.519295    48.60548     48.81757     46.279854  ]\n",
            " [ 88.96491     87.86698     91.473404    94.84867     91.54276\n",
            "   77.20444     79.93713     83.12551     86.283035    88.6323\n",
            "   82.2952      86.72619     81.8725      82.437515    78.29621\n",
            "   80.53653     78.2063      79.88629     89.8238      79.58758   ]\n",
            " [ 32.37862     28.28001     35.455605    26.838215    41.736534\n",
            "   31.238834    23.800137    22.7075      30.450966    26.898535\n",
            "   31.563684    30.338299    27.586983    23.696756    20.33666\n",
            "   25.659061    32.15494     28.92529     21.608559    28.686138  ]\n",
            " [ 23.864828    32.63576     34.45614     24.523287    33.40782\n",
            "   30.619787    32.631824    37.97182     27.227936    24.653889\n",
            "   23.943367    24.280926    27.944168    30.005796    29.654501\n",
            "   32.37862     28.28001     35.455605    26.838215    41.736534  ]\n",
            " [ 70.68671     65.02451     67.588295    63.59635     71.24777\n",
            "   60.667774    73.05737     63.8552      56.905815    64.7043\n",
            "   60.66297     68.27628     60.815144    61.852913    54.642056\n",
            "   61.834522    51.98331     56.19019     67.919136    68.053764  ]\n",
            " [ 65.80888     72.42587     64.03937     65.888794    67.51159\n",
            "   65.38417     62.19856     65.38135     69.759796    59.24037\n",
            "   65.01967     60.836647    70.29547     56.687828    61.493233\n",
            "   66.15852     72.08523     63.914536    61.453503    73.62201   ]\n",
            " [ 35.969654    21.868246    38.740505    18.333178    27.353228\n",
            "   31.072536    29.55496     25.05581     27.14825     25.743662\n",
            "   25.281895    32.497025    30.054634    24.825788    32.809357\n",
            "   29.869114    32.418423    31.52398     24.252817    25.619074  ]\n",
            " [ 41.78223     40.778324    31.413609    33.544636    36.22143\n",
            "   38.910835    31.098982    32.022236    26.868631    25.863333\n",
            "   35.345516    37.49725     29.783985    34.584694    30.793396\n",
            "   25.17319     29.614908    34.903008    26.434551    33.833897  ]\n",
            " [ 29.73035     23.368555    28.541082    28.048655    22.028439\n",
            "   29.518435    30.52015     33.118732    32.960247    20.791162\n",
            "   22.97916     30.232738    30.216269    30.212847    46.89202\n",
            "   30.474054    33.289154    32.373535    30.853155    26.012995  ]\n",
            " [ 47.394512    45.184433    49.935806    47.783203    53.519295\n",
            "   48.60548     48.81757     46.279854    47.845196    49.70109\n",
            "   39.64354     41.485905    51.975742    49.146484    47.405113\n",
            "   48.450214    50.12911     45.725693    44.566395    87.479225  ]\n",
            " [ 38.810547    37.214695    47.470825    31.43617     31.405706\n",
            "   30.38625     30.17146     29.214622    37.80533     38.427612\n",
            "   27.942549    26.112188    32.115417    26.488426    31.91241\n",
            "   29.26776     20.18714     35.023605    35.793327    23.285662  ]\n",
            " [ 49.154957    38.602703    45.406162    34.208767    37.00193\n",
            "   44.25351     46.578594    43.35101     41.514812    40.177532\n",
            "   33.871693    37.237713    38.097908    45.242546    41.222683\n",
            "   30.229015    40.196377    36.174564    34.231815    40.183838  ]\n",
            " [ 21.868246    38.740505    18.333178    27.353228    31.072536\n",
            "   29.55496     25.05581     27.14825     25.743662    25.281895\n",
            "   32.497025    30.054634    24.825788    32.809357    29.869114\n",
            "   32.418423    31.52398     24.252817    25.619074    32.178864  ]\n",
            " [ 21.541426    35.955315    20.655773    25.61978     28.487524\n",
            "   35.054302    20.683037    33.692867    27.943205    22.999773\n",
            "   30.233395    28.934015    24.953756    28.320253    26.06135\n",
            "   28.57245     31.332735    35.969654    21.868246    38.740505  ]\n",
            " [ 14.683323     5.5103145    5.3133783    3.0449743   -6.7815113\n",
            "    2.0786734    1.9152563   13.337309    -0.52937263   1.3560262\n",
            "   -0.90470564  -7.1490674    3.8389826    1.320357     0.95711476\n",
            "   -8.09809      2.9155295  -11.649013    -2.2406242    5.248558  ]\n",
            " [ 27.942549    26.112188    32.115417    26.488426    31.91241\n",
            "   29.26776     20.18714     35.023605    35.793327    23.285662\n",
            "   23.954716    26.874273    26.67722     27.866035    34.201256\n",
            "   22.851107    19.33878     16.037046    18.891464    22.002163  ]\n",
            " [ 30.694183    30.730724    27.001482    27.211473    30.757544\n",
            "   22.38766     22.621704    26.097902    28.654142    31.307323\n",
            "   37.161125    34.104355    29.048223    29.784843    24.899433\n",
            "   29.85182     28.533539    31.623013    25.906002    32.67176   ]\n",
            " [ 47.770836    50.988007    38.8417      49.86395     43.793186\n",
            "   49.91921     43.210567    37.963825    38.81263     47.153587\n",
            "   48.17531     42.31936     49.998848    38.46393     46.40746\n",
            "   40.649796    43.413387    46.87793     42.307915    44.656895  ]\n",
            " [ 28.934015    24.953756    28.320253    26.06135     28.57245\n",
            "   31.332735    35.969654    21.868246    38.740505    18.333178\n",
            "   27.353228    31.072536    29.55496     25.05581     27.14825\n",
            "   25.743662    25.281895    32.497025    30.054634    24.825788  ]\n",
            " [ 94.98353     81.38736     87.288605    77.69104     80.686905\n",
            "   89.36113     77.00803     88.83144     82.92502     76.18184\n",
            "   81.7551      83.04583     80.873344    69.48581     73.26541\n",
            "   75.219345    75.63242     78.64758     75.96446     67.923256  ]\n",
            " [ 46.418854    43.886986    43.061607    47.179943    45.395092\n",
            "   54.212364    33.46019     52.195435    52.991413    36.417343\n",
            "   45.094387    44.97436     39.817963    42.99061     41.351192\n",
            "   55.690018    51.631855    53.336246    50.61204     41.383835  ]\n",
            " [ 40.177532    33.871693    37.237713    38.097908    45.242546\n",
            "   41.222683    30.229015    40.196377    36.174564    34.231815\n",
            "   40.183838    41.78223     40.778324    31.413609    33.544636\n",
            "   36.22143     38.910835    31.098982    32.022236    26.868631  ]], shape=(32, 20), dtype=float32)\n",
            "\n",
            "32 labels\n",
            "tf.Tensor(\n",
            "[ 60.53453   17.424019  35.054302 -13.030719  44.97436   49.94979\n",
            "  50.150696  46.375916  32.960247  20.599699  47.845196  97.26462\n",
            "  29.73035   31.238834  66.2601    56.963406  32.178864  12.305289\n",
            "  31.377792  81.65221   23.954716  41.78223   31.51689   18.333178\n",
            " -11.178716  30.77161   37.77174   51.5815    32.809357  76.01741\n",
            "  44.43211   25.863333], shape=(32,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1Hl39rklkLm",
        "outputId": "65942aec-fa52-4fc6-d49d-80b4989d9938",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(51)\n",
        "np.random.seed(51)\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n",
        "                      input_shape=[None]),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "  tf.keras.layers.Dense(1),\n",
        "  tf.keras.layers.Lambda(lambda x: x * 100.0)\n",
        "])\n",
        "\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: 1e-8 * 10**(epoch / 20))\n",
        "optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n",
        "model.compile(loss=tf.keras.losses.Huber(),\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"mae\"])\n",
        "history = model.fit(dataset, epochs=100, callbacks=[lr_schedule])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "31/31 [==============================] - 1s 24ms/step - loss: 21.5167 - mae: 22.0115\n",
            "Epoch 2/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 21.1522 - mae: 21.6444\n",
            "Epoch 3/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 20.7302 - mae: 21.2236\n",
            "Epoch 4/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 20.2834 - mae: 20.7757\n",
            "Epoch 5/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 19.7897 - mae: 20.2855\n",
            "Epoch 6/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 19.1782 - mae: 19.6696\n",
            "Epoch 7/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 18.2410 - mae: 18.7333\n",
            "Epoch 8/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 17.4825 - mae: 17.9748\n",
            "Epoch 9/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 17.1597 - mae: 17.6529\n",
            "Epoch 10/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 16.8547 - mae: 17.3463\n",
            "Epoch 11/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 16.5533 - mae: 17.0471\n",
            "Epoch 12/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 16.2565 - mae: 16.7524\n",
            "Epoch 13/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 15.9629 - mae: 16.4598\n",
            "Epoch 14/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 15.6593 - mae: 16.1548\n",
            "Epoch 15/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 15.3713 - mae: 15.8652\n",
            "Epoch 16/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 15.0752 - mae: 15.5697\n",
            "Epoch 17/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 14.7905 - mae: 15.2869\n",
            "Epoch 18/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 14.5229 - mae: 15.0178\n",
            "Epoch 19/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 14.2707 - mae: 14.7636\n",
            "Epoch 20/100\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 14.0293 - mae: 14.5217\n",
            "Epoch 21/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 13.8191 - mae: 14.3135\n",
            "Epoch 22/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 13.6084 - mae: 14.1032\n",
            "Epoch 23/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 13.4096 - mae: 13.9038\n",
            "Epoch 24/100\n",
            "31/31 [==============================] - 1s 25ms/step - loss: 13.1859 - mae: 13.6794\n",
            "Epoch 25/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 12.9819 - mae: 13.4747\n",
            "Epoch 26/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 12.7278 - mae: 13.2184\n",
            "Epoch 27/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 12.4886 - mae: 12.9799\n",
            "Epoch 28/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 12.3459 - mae: 12.8362\n",
            "Epoch 29/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 11.9127 - mae: 12.4017\n",
            "Epoch 30/100\n",
            "31/31 [==============================] - 1s 24ms/step - loss: 11.5934 - mae: 12.0835\n",
            "Epoch 31/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 11.2859 - mae: 11.7771\n",
            "Epoch 32/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 11.3520 - mae: 11.8418\n",
            "Epoch 33/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 10.5232 - mae: 11.0140\n",
            "Epoch 34/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 10.5483 - mae: 11.0364\n",
            "Epoch 35/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 10.0639 - mae: 10.5494\n",
            "Epoch 36/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 10.5303 - mae: 11.0200\n",
            "Epoch 37/100\n",
            "31/31 [==============================] - 1s 24ms/step - loss: 10.1915 - mae: 10.6815\n",
            "Epoch 38/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 9.6502 - mae: 10.1381\n",
            "Epoch 39/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 9.1531 - mae: 9.6376\n",
            "Epoch 40/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 8.7075 - mae: 9.1934\n",
            "Epoch 41/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 8.2888 - mae: 8.7743\n",
            "Epoch 42/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 7.9466 - mae: 8.4313\n",
            "Epoch 43/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 7.6212 - mae: 8.1052\n",
            "Epoch 44/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 7.3689 - mae: 7.8547\n",
            "Epoch 45/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 7.1997 - mae: 7.6829\n",
            "Epoch 46/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 6.8836 - mae: 7.3661\n",
            "Epoch 47/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 6.6758 - mae: 7.1594\n",
            "Epoch 48/100\n",
            "31/31 [==============================] - 1s 24ms/step - loss: 6.5791 - mae: 7.0609\n",
            "Epoch 49/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 6.2076 - mae: 6.6838\n",
            "Epoch 50/100\n",
            "31/31 [==============================] - 1s 24ms/step - loss: 6.0389 - mae: 6.5179\n",
            "Epoch 51/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 5.9099 - mae: 6.3876\n",
            "Epoch 52/100\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 5.7769 - mae: 6.2586\n",
            "Epoch 53/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 5.6296 - mae: 6.1098\n",
            "Epoch 54/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 5.5207 - mae: 5.9948\n",
            "Epoch 55/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 5.3915 - mae: 5.8635\n",
            "Epoch 56/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 5.5568 - mae: 6.0306\n",
            "Epoch 57/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 5.2790 - mae: 5.7568\n",
            "Epoch 58/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 5.2576 - mae: 5.7360\n",
            "Epoch 59/100\n",
            "31/31 [==============================] - 1s 24ms/step - loss: 5.1189 - mae: 5.5976\n",
            "Epoch 60/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 5.0482 - mae: 5.5221\n",
            "Epoch 61/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 5.3073 - mae: 5.7862\n",
            "Epoch 62/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 5.2209 - mae: 5.7006\n",
            "Epoch 63/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 5.0224 - mae: 5.4971\n",
            "Epoch 64/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 4.8378 - mae: 5.3121\n",
            "Epoch 65/100\n",
            "31/31 [==============================] - 1s 24ms/step - loss: 5.2828 - mae: 5.7625\n",
            "Epoch 66/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 5.3684 - mae: 5.8518\n",
            "Epoch 67/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 6.1172 - mae: 6.5999\n",
            "Epoch 68/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 5.2448 - mae: 5.7288\n",
            "Epoch 69/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 5.4579 - mae: 5.9387\n",
            "Epoch 70/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 4.8784 - mae: 5.3517\n",
            "Epoch 71/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 5.8512 - mae: 6.3328\n",
            "Epoch 72/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 5.7091 - mae: 6.1906\n",
            "Epoch 73/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 6.5024 - mae: 6.9855\n",
            "Epoch 74/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 5.1171 - mae: 5.5983\n",
            "Epoch 75/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 5.4665 - mae: 5.9459\n",
            "Epoch 76/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 5.5865 - mae: 6.0675\n",
            "Epoch 77/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 6.6011 - mae: 7.0869\n",
            "Epoch 78/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 5.3414 - mae: 5.8222\n",
            "Epoch 79/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 5.3523 - mae: 5.8316\n",
            "Epoch 80/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 5.1324 - mae: 5.6126\n",
            "Epoch 81/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 7.2442 - mae: 7.7277\n",
            "Epoch 82/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 5.2931 - mae: 5.7740\n",
            "Epoch 83/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 6.1063 - mae: 6.5905\n",
            "Epoch 84/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 8.0418 - mae: 8.5283\n",
            "Epoch 85/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 6.0476 - mae: 6.5356\n",
            "Epoch 86/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 7.0666 - mae: 7.5547\n",
            "Epoch 87/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 7.2429 - mae: 7.7278\n",
            "Epoch 88/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 6.3571 - mae: 6.8373\n",
            "Epoch 89/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 7.0662 - mae: 7.5513\n",
            "Epoch 90/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 7.5634 - mae: 8.0461\n",
            "Epoch 91/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 7.4016 - mae: 7.8897\n",
            "Epoch 92/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 10.3108 - mae: 10.8019\n",
            "Epoch 93/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 8.9524 - mae: 9.4390\n",
            "Epoch 94/100\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 9.0059 - mae: 9.4936\n",
            "Epoch 95/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 11.7405 - mae: 12.2335\n",
            "Epoch 96/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 10.1950 - mae: 10.6860\n",
            "Epoch 97/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 7.8055 - mae: 8.2923\n",
            "Epoch 98/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 8.5342 - mae: 9.0218\n",
            "Epoch 99/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 8.4686 - mae: 8.9568\n",
            "Epoch 100/100\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 13.1609 - mae: 13.6531\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkBsrsXMzoWR",
        "outputId": "99a3f57b-9145-4802-8256-401a54b238e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        }
      },
      "source": [
        "plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
        "plt.axis([1e-8, 1e-4, 0, 30])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1e-08, 0.0001, 0, 30]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uh-97bpLZCA"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(51)\n",
        "np.random.seed(51)\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n",
        "                      input_shape=[None]),\n",
        "   tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "  tf.keras.layers.Dense(1),\n",
        "  tf.keras.layers.Lambda(lambda x: x * 100.0)\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(lr=1e-5, momentum=0.9),metrics=[\"mae\"])\n",
        "history = model.fit(dataset,epochs=500,verbose=0)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icGDaND7z0ne",
        "outputId": "f01b98ed-c279-4dda-8587-4c04a512ce2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "source": [
        "forecast = []\n",
        "results = []\n",
        "for time in range(len(series) - window_size):\n",
        "  forecast.append(model.predict(series[time:time + window_size][np.newaxis]))\n",
        "\n",
        "forecast = forecast[split_time-window_size:]\n",
        "results = np.array(forecast)[:, 0, 0]\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "plot_series(time_valid, x_valid)\n",
        "plot_series(time_valid, results)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ff78acc23dfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mforecast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mforecast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtime\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'series' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfPeqI7rz4LD",
        "outputId": "bb37ff3e-35c3-474f-fd15-56c2affe52dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.keras.metrics.mean_absolute_error(x_valid, results).numpy()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.450017"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUsdZB_tzDLe",
        "outputId": "964c47d9-7eec-42cd-905f-c45c2fd31d9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "import matplotlib.image  as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#-----------------------------------------------------------\n",
        "# Retrieve a list of list results on training and test data\n",
        "# sets for each training epoch\n",
        "#-----------------------------------------------------------\n",
        "mae=history.history['mae']\n",
        "loss=history.history['loss']\n",
        "\n",
        "epochs=range(len(loss)) # Get number of epochs\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot MAE and Loss\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, mae, 'r')\n",
        "plt.plot(epochs, loss, 'b')\n",
        "plt.title('MAE and Loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend([\"MAE\", \"Loss\"])\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "epochs_zoom = epochs[200:]\n",
        "mae_zoom = mae[200:]\n",
        "loss_zoom = loss[200:]\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot Zoomed MAE and Loss\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs_zoom, mae_zoom, 'r')\n",
        "plt.plot(epochs_zoom, loss_zoom, 'b')\n",
        "plt.title('MAE and Loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend([\"MAE\", \"Loss\"])\n",
        "\n",
        "plt.figure()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-0d8269471fd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# sets for each training epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#-----------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmae\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mae'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'mae'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CGaYFxXNEAK"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n",
        "                      input_shape=[None]),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "  tf.keras.layers.Dense(1),\n",
        "  tf.keras.layers.Lambda(lambda x: x * 100.0)\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9))\n",
        "model.fit(dataset,epochs=100, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ3R8ysauz9e"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n",
        "                      input_shape=[None]),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "  tf.keras.layers.Dense(1),\n",
        "  tf.keras.layers.Lambda(lambda x: x * 100.0)\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9))\n",
        "model.fit(dataset,epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}